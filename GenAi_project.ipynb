{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end data engineering project using generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "<summary>Click for Learning Objectives</summary>\n",
    "<ol>\n",
    "\n",
    "- Data architecture design\n",
    "- Data warehouse and schema design\n",
    "- Infrastructure requirements determination\n",
    "- ETL pipeline integration\n",
    "- Querying databases\n",
    "- Data analysis and mining\n",
    "\n",
    "</ol>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Enhancing Marketing Strategies through Generative AI Applications__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://imgur.com/jI1lf4q.png\" />\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "<ol>\n",
    "<li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Dataset\">Dataset\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Scenario\">Scenario\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Data-Architecture\">Data Architecture\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Data-Warehouse-Schema-and-ERD\">Data Warehouse Schema and ERD\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Infrastructure-Requirements\">Infrastructure Requirements\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#ETL-Workflow\">ETL Workflow\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Database-Queries\">Database Queries\n",
    "    </a>\n",
    "  </li>\n",
    "<li>\n",
    "    <a href=\"#Data-Analysis-and-Mining\">Data Analysis and Mining\n",
    "    </a>\n",
    "  </li>        \n",
    "<li>\n",
    "    <a href=\"#Conclusion\">Conclusion\n",
    "    </a>\n",
    "  </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Setup__\n",
    "\n",
    "\n",
    "For this project, we'll be using the following libraries:\n",
    "\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [numpy](https://numpy.org/)\n",
    "* [scikit-learn](https://scikit-learn.org/)\n",
    "* [mlxtend](https://rasbt.github.io/mlxtend/)\n",
    "\n",
    "and outputs from both [gpt-4](https://openai.com/index/gpt-4/) and [gemini](https://gemini.google.com/) for the [prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering) sections of the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to suppress warnings generated by the code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the '#' and run this cell, if using a different environment\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Dataset__\n",
    "[Online retail data set](https://archive.ics.uci.edu/dataset/352/online+retail) available in the UCI ML library, available publically under the[ CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Scenario__\n",
    " You are a data engineer hired by an online retail company to design a data workflow for their operations. You are required to perform all of the following tasks for them:\n",
    "\n",
    "* Propose a detailed data architecture for the whole data process.\n",
    "  * The client does not want cloud-based processing resources. The company wants an SQL-based central data repository that their employees from multiple countries can access for their use.\n",
    "\n",
    "* Propose a detailed data warehouse schema and design its entity requirements document (ERD).\n",
    "  * The client wants customer information, seller information, inventory information, and transaction invoice information, to be recorded.\n",
    "  * The client wants the final data prepared such that the final record of sales invoices contains the headers InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.\n",
    "\n",
    "* Recommend the infrastructure requirements for the proposed data architecture.\n",
    "* Create an ETL pipeline to clean, process, and load the data to an SQL server for analysis. Test the pipeline on a sample database.\n",
    "  * The recorded data is available at a provided URL.\n",
    "  * The InvoiceNo starting with the character C is a credit entry, and you should remove these entries from the records before starting your analysis.\n",
    "  * The StockCode values of C2, D, M, and POST correspond to Carraige, Discount, Manual, and Postage entries, none of which are required for analysis.\n",
    "  * The CustomerID is missing from a few entries. Remove these entries from the data before analysis.\n",
    "  * Load the final transaction record to an SQLite3 database Invoice_Records under the table Purchase_transactions.\n",
    "\n",
    "* Query the SQL database to access data from the server.\n",
    "* Implement data analysis and data mining strategies on the final data.\n",
    "  * Then, implement Apriori algorithm and perform association rule mining on the data for the specified country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Project Proposal</summary>\n",
    "\n",
    "#### Comprehensive Data Engineering Solution\n",
    "\n",
    "To optimize your company's operations and drive data-driven decision-making, we propose a comprehensive data engineering solution. Our services will encompass:\n",
    "\n",
    "* **Development of a robust data architecture:** Tailored to your specific business needs and operational requirements. \n",
    "\n",
    "* **Design and implementation of a data warehouse schema:** Including a detailed Entity Relationship Diagram (ERD) to visualize data relationships.\n",
    "\n",
    "* **Assessment and recommendation of optimal infrastructure:** To support efficient data storage, processing, and analysis.\n",
    "\n",
    "* **Creation of a robust ETL pipeline:** To extract, transform, and load data into a SQL server for analysis. Rigorous testing will be conducted to ensure data integrity and pipeline reliability.\n",
    "\n",
    "* **Query development:** To enable seamless data access and retrieval from the SQL server.\n",
    "\n",
    "* **Implementation of advanced data analytics and mining techniques:** To uncover valuable insights and patterns within your data. \n",
    "\n",
    "By delivering these services, we aim to empower your organization with actionable intelligence and a solid foundation for data-driven success.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Architecture__\n",
    "<p>\n",
    "Using GenAI model to propose, the following prompt can give us a comprehensive data architecture that meets the specifications of not using a cloud platform and a SQL-based central repository.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "    <summary>GenAI prompt</summary>\n",
    "    \n",
    "```python\n",
    "Create a data architecture, clearly defining each step of the process, for an online retail company.    \n",
    "Please take note of the following specifications:\n",
    "The company does not want to use cloud platforms for storage or processing.\n",
    "They want an SQL-based central data repository that can be accessed by any employee as per requirements.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click for GPT-4</summary>\n",
    "\n",
    "Certainly! Below is a detailed data architecture for an online retail company that meets the specified requirements:\n",
    "\n",
    "### 1. Data Ingestion\n",
    "\n",
    "**Sources of Data:**\n",
    "- **Website and Mobile App**: Customer interactions, orders, product views, etc.\n",
    "- **Point of Sale (POS) Systems**: In-store transactions.\n",
    "- **Third-Party Services**: Payment gateways, shipping providers, etc.\n",
    "- **Internal Systems**: Inventory management, CRM, etc.\n",
    "\n",
    "**Data Ingestion Tools:**\n",
    "- **ETL (Extract, Transform, Load) Tools**: Tools like Apache NiFi or Talend can be used to extract data from various sources, transform it as needed, and load it into the central repository.\n",
    "- **Custom Scripts**: Python or Java scripts can be used for custom data ingestion processes.\n",
    "\n",
    "### 2. Data Storage\n",
    "\n",
    "**Central Data Repository:**\n",
    "- **SQL Database**: Use a robust SQL-based database like PostgreSQL or MySQL for storing all the ingested data. Ensure high availability and redundancy through replication.\n",
    "\n",
    "**Data Organization:**\n",
    "- **Schema Design**: Design the database schema to efficiently store data from different sources. Use normalization to reduce redundancy and improve data integrity.\n",
    "- **Tables**: Customers, Orders, Products, Inventory, Transactions, etc.\n",
    "- **Indexes**: Create indexes on frequently queried columns to improve query performance.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "**Batch Processing:**\n",
    "- **Scheduled Jobs**: Use cron jobs or a scheduling tool like Apache Airflow to run batch processing tasks. These tasks can include data cleaning, aggregation, and transformation.\n",
    "\n",
    "**Real-Time Processing:**\n",
    "- **Message Queues**: Use message queuing systems like Kafka or RabbitMQ to handle real-time data streams.\n",
    "- **Stream Processing**: Use Apache Flink or Apache Storm for real-time data processing and analytics.\n",
    "\n",
    "### 4. Data Access\n",
    "\n",
    "**Data Access Layer:**\n",
    "- **SQL Queries**: Employees can use SQL queries to access the data they need. Provide training and documentation to ensure they can write effective queries.\n",
    "- **BI Tools**: Integrate Business Intelligence tools like Metabase or Tableau (on-premise versions) for non-technical users to create reports and dashboards.\n",
    "\n",
    "**Access Control:**\n",
    "- **Role-Based Access Control (RBAC)**: Implement RBAC to ensure that employees can only access the data they are authorized to see.\n",
    "- **Audit Logs**: Maintain logs of data access and modifications for security and compliance purposes.\n",
    "\n",
    "### 5. Data Security\n",
    "\n",
    "**Encryption:**\n",
    "- **At Rest**: Use disk encryption to protect data stored in the SQL database.\n",
    "- **In Transit**: Use SSL/TLS to encrypt data transmitted between the database and client applications.\n",
    "\n",
    "**Backup and Recovery:**\n",
    "- **Regular Backups**: Schedule regular backups of the SQL database to prevent data loss.\n",
    "- **Disaster Recovery Plan**: Implement a disaster recovery plan to quickly restore data in case of hardware failure or other issues.\n",
    "\n",
    "### 6. Data Governance\n",
    "\n",
    "**Data Quality:**\n",
    "- **Validation Rules**: Implement validation rules to ensure data quality during ingestion.\n",
    "- **Data Audits**: Regularly audit data for accuracy and completeness.\n",
    "\n",
    "**Compliance:**\n",
    "- **Regulatory Compliance**: Ensure that data storage and processing comply with relevant regulations (e.g., GDPR, CCPA).\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Data Ingestion**: Use ETL tools and custom scripts to collect data from various sources.\n",
    "2. **Data Storage**: Store data in a robust SQL database like PostgreSQL or MySQL.\n",
    "3. **Data Processing**: Use batch processing for scheduled tasks and real-time processing tools for live data.\n",
    "4. **Data Access**: Provide SQL query access and BI tools for data retrieval and analysis.\n",
    "5. **Data Security**: Implement encryption, access control, and regular backups.\n",
    "6. **Data Governance**: Ensure data quality and compliance with regulatory standards.\n",
    "\n",
    "This architecture ensures a robust, secure, and efficient data management system that meets the company's requirements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click for Gemini</summary>\n",
    "\n",
    "## Data Architecture for an On-Premise Online Retail Company\n",
    "\n",
    "### Understanding the Constraints\n",
    "Before delving into the architecture, it's crucial to acknowledge the specific requirements:\n",
    "\n",
    "* **No cloud platforms:** This necessitates on-premise infrastructure and data management solutions.\n",
    "* **SQL-based central data repository:** This implies a relational database as the core data storage component.\n",
    "* **Accessible to all employees:** This mandates robust security measures to protect sensitive data while allowing appropriate access.\n",
    "\n",
    "### Proposed Data Architecture\n",
    "\n",
    "#### 1. Data Generation\n",
    "* **Point of Sale (POS) Systems:** These systems generate transactional data for in-store purchases.\n",
    "* **E-commerce Platform:** This platform captures online orders, customer information, product views, and other relevant data.\n",
    "* **Inventory Management System:** Tracks product stock levels, replenishment needs, and location information.\n",
    "* **Customer Relationship Management (CRM) System:** Stores customer data, interactions, and preferences.\n",
    "\n",
    "#### 2. Data Extraction, Transformation, and Loading (ETL)\n",
    "* **Data Extraction:** Data is extracted from various source systems using specialized tools or custom-built scripts.\n",
    "* **Data Transformation:** Extracted data undergoes cleaning, standardization, and transformation to ensure data quality and consistency. This might involve data validation, enrichment, and aggregation.\n",
    "* **Data Loading:** Cleansed and transformed data is loaded into the central data repository.\n",
    "\n",
    "#### 3. Central Data Repository\n",
    "* **Relational Database Management System (RDBMS):** A powerful RDBMS (e.g., Oracle, SQL Server, PostgreSQL) is used as the core data warehouse. This database should be optimized for analytical workloads.\n",
    "* **Data Modeling:** A robust data model is designed to accommodate various data types (transactional, customer, product, etc.) and support complex queries.\n",
    "* **Data Partitioning:** To improve query performance and manageability, data can be partitioned based on time, geography, or other relevant criteria.\n",
    "* **Data Replication:** For high availability and disaster recovery, consider implementing data replication mechanisms.\n",
    "\n",
    "#### 4. Data Access and Utilization\n",
    "* **Business Intelligence (BI) Tools:** Tools like Tableau, Power BI, or QlikView can be used to create interactive dashboards and reports for business users.\n",
    "* **Data Visualization:** Visual representations of data can provide insights and trends.\n",
    "* **Ad-hoc Querying:** Employees can use SQL or BI tools to conduct ad-hoc analysis based on their specific needs.\n",
    "* **Data Security:** Implement robust security measures to protect sensitive data, including access controls, encryption, and data masking.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "* **Hardware Infrastructure:** Sufficient compute, storage, and networking resources are required to support the data architecture.\n",
    "* **Data Quality:** Establish data quality standards and processes to ensure data accuracy and consistency.\n",
    "* **Data Governance:** Define data ownership, stewardship, and usage policies to maintain data integrity and compliance.\n",
    "* **Scalability:** The architecture should be designed to accommodate future growth in data volume and complexity.\n",
    "* **Performance:** Optimize database design, indexing, and query performance to support efficient data access.\n",
    "* **Data Backup and Recovery:** Implement regular backups and disaster recovery plans to protect against data loss.\n",
    "\n",
    "### Additional Considerations\n",
    "* **Data Integration Challenges:** Integrating data from diverse sources might require complex ETL processes and data mapping.\n",
    "* **Data Volume:** For large-scale retailers, data volumes can be immense, requiring efficient storage and processing capabilities.\n",
    "* **Real-time Analytics:** If real-time insights are crucial, consider using data streaming technologies and in-memory databases.\n",
    "\n",
    "By carefully addressing these factors and following the outlined architecture, an online retail company can establish a solid foundation for data-driven decision making.\n",
    " \n",
    "**Would you like to delve deeper into a specific component of this architecture or discuss potential challenges and solutions?**\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Warehouse Schema and ERD__\n",
    "\n",
    "Let's set up the data warehouse schema and its ERD diagram defined requirements from the client as to the kind of data they want recorded. Assume that you received the following information from the client.\n",
    "\n",
    "- The client wants to record customer information, seller information, inventory information, and transaction invoice information.\n",
    "\n",
    "- The client wants the final data prepared such that the final record of sales invoices displayes the headers InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID and Country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "    <summary>GenAI prompt</summary>\n",
    "\n",
    "```python\n",
    "Propose a data schema for an online retail store that meets the following guidelines.\n",
    "Records Customer information, inventory information, and transaction invoice information.\n",
    "Prepare the final data such that the final record of sales invoices has the headers `InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID` and `Country`.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>GPT-4</summary>\n",
    "\n",
    "Certainly! Below is a proposed data schema for an online retail store that records customer information, inventory information, and transaction invoice information. The final sales invoice record includes the specified headers.\n",
    "\n",
    "### Data Schema\n",
    "\n",
    "#### 1. Customers Table\n",
    "\n",
    "**Table Name:** `Customers`\n",
    "\n",
    "| Column Name | Data Type | Constraints              |\n",
    "|-------------|------------|--------------------------|\n",
    "| CustomerID  | INT        | PRIMARY KEY, AUTO_INCREMENT |\n",
    "| FirstName   | VARCHAR(50)| NOT NULL                 |\n",
    "| LastName    | VARCHAR(50)| NOT NULL                 |\n",
    "| Email       | VARCHAR(100)| UNIQUE, NOT NULL        |\n",
    "| PhoneNumber | VARCHAR(20)|                          |\n",
    "| Address     | VARCHAR(255)|                         |\n",
    "| City        | VARCHAR(50)|                          |\n",
    "| State       | VARCHAR(50)|                          |\n",
    "| Country     | VARCHAR(50)|                          |\n",
    "| PostalCode  | VARCHAR(20)|                          |\n",
    "| CreatedAt   | TIMESTAMP  | DEFAULT CURRENT_TIMESTAMP|\n",
    "\n",
    "#### 2. Inventory Table\n",
    "\n",
    "**Table Name:** `Inventory`\n",
    "\n",
    "| Column Name | Data Type | Constraints              |\n",
    "|-------------|------------|--------------------------|\n",
    "| StockCode   | VARCHAR(20)| PRIMARY KEY              |\n",
    "| Description | VARCHAR(255)| NOT NULL                |\n",
    "| UnitPrice   | DECIMAL(10, 2)| NOT NULL             |\n",
    "| QuantityInStock | INT     | NOT NULL                |\n",
    "| LastRestocked | DATE     |                          |\n",
    "\n",
    "#### 3. Transactions Table\n",
    "\n",
    "**Table Name:** `Transactions`\n",
    "\n",
    "| Column Name | Data Type | Constraints              |\n",
    "|-------------|------------|--------------------------|\n",
    "| InvoiceNo   | INT        | PRIMARY KEY, AUTO_INCREMENT |\n",
    "| InvoiceDate | TIMESTAMP  | DEFAULT CURRENT_TIMESTAMP |\n",
    "| CustomerID  | INT        | FOREIGN KEY REFERENCES Customers(CustomerID) |\n",
    "| TotalAmount | DECIMAL(10, 2)| NOT NULL             |\n",
    "| Country     | VARCHAR(50)| NOT NULL                |\n",
    "\n",
    "#### 4. TransactionDetails Table\n",
    "\n",
    "**Table Name:** `TransactionDetails`\n",
    "\n",
    "| Column Name | Data Type | Constraints              |\n",
    "|-------------|------------|--------------------------|\n",
    "| DetailID    | INT        | PRIMARY KEY, AUTO_INCREMENT |\n",
    "| InvoiceNo   | INT        | FOREIGN KEY REFERENCES Transactions(InvoiceNo) |\n",
    "| StockCode   | VARCHAR(20)| FOREIGN KEY REFERENCES Inventory(StockCode) |\n",
    "| Quantity    | INT        | NOT NULL                |\n",
    "| UnitPrice   | DECIMAL(10, 2)| NOT NULL             |\n",
    "\n",
    "### Final Sales Invoice Record\n",
    "\n",
    "To prepare the final sales invoice record with the headers `InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID`, and `Country`, you can use the following SQL query:\n",
    "\n",
    "    ``` sql\n",
    "    SELECT \n",
    "        t.InvoiceNo,\n",
    "        td.StockCode,\n",
    "        i.Description,\n",
    "        td.Quantity,\n",
    "        t.InvoiceDate,\n",
    "        td.UnitPrice,\n",
    "        t.CustomerID,\n",
    "        t.Country\n",
    "    FROM \n",
    "        Transactions t\n",
    "    JOIN \n",
    "        TransactionDetails td ON t.InvoiceNo = td.InvoiceNo\n",
    "    JOIN \n",
    "        Inventory i ON td.StockCode = i.StockCode;\n",
    "    ```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Customers Table**: Stores customer information including personal details and contact information.\n",
    "2. **Inventory Table**: Stores inventory details including stock code, product description, unit price, and quantity in stock.\n",
    "3. **Transactions Table**: Stores transaction header information including invoice number, date, customer ID, total amount, and country.\n",
    "4. **TransactionDetails Table**: Stores details of each transaction including invoice number, stock code, quantity, and unit price.\n",
    "\n",
    "By joining the `Transactions`, `TransactionDetails`, and `Inventory` tables, the final sales invoice record can be generated with the required headers. This schema ensures normalization and efficient data retrieval.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Gemini</summary>\n",
    "\n",
    "## Proposed Data Schema for Online Retail Store\n",
    "\n",
    "### Understanding the Requirements\n",
    "Based on the given requirements, we need to store customer information, inventory information, and transaction invoice information. The final output should be a sales invoice record with the specified columns.\n",
    "\n",
    "### Proposed Schema\n",
    "\n",
    "#### Tables\n",
    "**1. Customers**\n",
    "* CustomerID (Primary Key)\n",
    "* CustomerName\n",
    "* Address\n",
    "* City\n",
    "* Postcode\n",
    "* Country\n",
    "* Phone\n",
    "* Email\n",
    "\n",
    "**2. Products**\n",
    "* StockCode (Primary Key)\n",
    "* Description\n",
    "* UnitPrice\n",
    "\n",
    "    **3. Orders**\n",
    "    * OrderID (Primary Key)\n",
    "    * OrderDate\n",
    "    * CustomerID (Foreign Key referencing Customers)\n",
    "    * ShipName\n",
    "    * ShipAddress\n",
    "    * ShipCity\n",
    "    * ShipPostcode\n",
    "    * ShipCountry\n",
    "    **4. OrderDetails**\n",
    "    * OrderID (Foreign Key referencing Orders)\n",
    "    * StockCode (Foreign Key referencing Products)\n",
    "    * Quantity\n",
    "    * UnitPrice (Redundant, can be derived from Products)\n",
    "    * Discount (Optional)\n",
    "\n",
    "    ### Explanation of the Schema\n",
    "\n",
    "    * **Customers** table stores basic customer information.\n",
    "    * **Products** table stores product details like StockCode, Description, and UnitPrice.\n",
    "    * **Orders** table stores order-level information like order date, shipping details, and customer ID.\n",
    "    * **OrderDetails** table stores details of each product in an order, including quantity and price.\n",
    "\n",
    "    ### Deriving the Final Sales Invoice Record\n",
    "\n",
    "    The final sales invoice record can be derived through a SQL query that joins the `Orders`, `OrderDetails`, and `Products` tables. Here's a basic example:\n",
    "\n",
    "    ```sql\n",
    "    SELECT \n",
    "        o.OrderID AS InvoiceNo,\n",
    "        od.StockCode,\n",
    "        p.Description,\n",
    "        od.Quantity,\n",
    "        o.OrderDate AS InvoiceDate,\n",
    "        od.UnitPrice,\n",
    "        o.CustomerID,\n",
    "        o.ShipCountry AS Country\n",
    "    FROM Orders o\n",
    "    INNER JOIN OrderDetails od ON o.OrderID = od.OrderID\n",
    "    INNER JOIN Products p ON od.StockCode = p.StockCode;\n",
    "    ```\n",
    "\n",
    "    This query will produce a result set with the desired columns: `InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID`, and `Country`.\n",
    "\n",
    "    ### Additional Considerations\n",
    "    * **Indexes:** Creating appropriate indexes on frequently used columns (e.g., `OrderID`, `StockCode`, `CustomerID`) can improve query performance.\n",
    "    * **Data Types:** Ensure correct data types for columns (e.g., `CustomerID` as integer, `UnitPrice` as decimal).\n",
    "    * **Normalization:** The proposed schema is relatively normalized, but further normalization might be required depending on data complexity and specific needs.\n",
    "    * **Performance Optimization:** For large datasets, consider partitioning, indexing, and query optimization techniques.\n",
    "\n",
    "    By following this schema and considering the additional points, you can create a robust and efficient data structure for your online retail store.\n",
    "\n",
    "    **Would you like to explore any specific aspects of this schema in more detail, such as adding dimensions for reporting, or handling product variants?**\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "Let's also generate the SQL code to create this warehouse. \n",
    "You can use the SQL generated code to create an ERD on online tools such as [Lucidchart](https://lucidchart.com/), [DbDiagram](https://dbdiagram.io/), [draw.io](https://www.drawio.com/), just to name a few\n",
    "\n",
    "<details open>\n",
    "        <summary>GenAI prompt</summary>\n",
    "\n",
    "```python\n",
    "Generate the SQL code to create this data warehouse\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>GPT-4</summary>\n",
    "\n",
    "Certainly! Below is the SQL code to create the data warehouse schema for the online retail store, including the `Customers`, `Inventory`, `Transactions`, and `TransactionDetails` tables.\n",
    "\n",
    "```sql\n",
    "-- Create Customers Table\n",
    "CREATE TABLE Customers (\n",
    "    CustomerID INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    FirstName VARCHAR(50) NOT NULL,\n",
    "    LastName VARCHAR(50) NOT NULL,\n",
    "    Email VARCHAR(100) UNIQUE NOT NULL,\n",
    "    PhoneNumber VARCHAR(20),\n",
    "    Address VARCHAR(255),\n",
    "    City VARCHAR(50),\n",
    "    State VARCHAR(50),\n",
    "    Country VARCHAR(50),\n",
    "    PostalCode VARCHAR(20),\n",
    "    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Create Inventory Table\n",
    "CREATE TABLE Inventory (\n",
    "    StockCode VARCHAR(20) PRIMARY KEY,\n",
    "    Description VARCHAR(255) NOT NULL,\n",
    "    UnitPrice DECIMAL(10, 2) NOT NULL,\n",
    "    QuantityInStock INT NOT NULL,\n",
    "    LastRestocked DATE\n",
    ");\n",
    "\n",
    "-- Create Transactions Table\n",
    "CREATE TABLE Transactions (\n",
    "    InvoiceNo INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    InvoiceDate TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    CustomerID INT,\n",
    "    TotalAmount DECIMAL(10, 2) NOT NULL,\n",
    "    Country VARCHAR(50) NOT NULL,\n",
    "    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\n",
    ");\n",
    "\n",
    "-- Create TransactionDetails Table\n",
    "CREATE TABLE TransactionDetails (\n",
    "    DetailID INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    InvoiceNo INT,\n",
    "    StockCode VARCHAR(20),\n",
    "    Quantity INT NOT NULL,\n",
    "    UnitPrice DECIMAL(10, 2) NOT NULL,\n",
    "    FOREIGN KEY (InvoiceNo) REFERENCES Transactions(InvoiceNo),\n",
    "    FOREIGN KEY (StockCode) REFERENCES Inventory(StockCode)\n",
    ");\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Customers Table**:\n",
    "    - `CustomerID`: Primary key, auto-incremented.\n",
    "    - `FirstName`, `LastName`, `Email`: Basic customer information.\n",
    "    - `PhoneNumber`, `Address`, `City`, `State`, `Country`, `PostalCode`: Additional customer details.\n",
    "    - `CreatedAt`: Timestamp of when the customer record was created.\n",
    "\n",
    "2. **Inventory Table**:\n",
    "    - `StockCode`: Primary key, unique identifier for each product.\n",
    "    - `Description`: Description of the product.\n",
    "    - `UnitPrice`: Price per unit of the product.\n",
    "    - `QuantityInStock`: Quantity available in stock.\n",
    "    - `LastRestocked`: Date when the product was last restocked.\n",
    "\n",
    "3. **Transactions Table**:\n",
    "    - `InvoiceNo`: Primary key, auto-incremented.\n",
    "    - `InvoiceDate`: Timestamp of when the transaction took place.\n",
    "    - `CustomerID`: Foreign key linking to the `Customers` table.\n",
    "    - `TotalAmount`: Total amount of the transaction.\n",
    "    - `Country`: Country where the transaction took place.\n",
    "\n",
    "4. **TransactionDetails Table**:\n",
    "    - `DetailID`: Primary key, auto-incremented.\n",
    "    - `InvoiceNo`: Foreign key linking to the `Transactions` table.\n",
    "    - `StockCode`: Foreign key linking to the `Inventory` table.\n",
    "    - `Quantity`: Quantity of the product in the transaction.\n",
    "    - `UnitPrice`: Price per unit for the product in the transaction.\n",
    "\n",
    "This SQL code will create the necessary tables for the data warehouse schema, ensuring that the relationships between customers, inventory, transactions, and transaction details are properly established.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://imgur.com/IWkBy0n.png\" />\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Gemini</summary>\n",
    "\n",
    "\n",
    "## Creating the SQL Code for the Online Retail Data Warehouse\n",
    "\n",
    "**Understanding the Requirements:**\n",
    "\n",
    "Before we generate the SQL code, let's recap the table structure we defined:\n",
    "\n",
    "* **Customers:** CustomerID (PK), CustomerName, Address, City, Postcode, Country, Phone, Email\n",
    "* **Products:** StockCode (PK), Description, UnitPrice\n",
    "* **Orders:** OrderID (PK), OrderDate, CustomerID (FK), ShipName, ShipAddress, ShipCity, ShipPostcode, ShipCountry\n",
    "* **OrderDetails:** OrderID (FK), StockCode (FK), Quantity, UnitPrice (redundant), Discount (optional)\n",
    "\n",
    "**Assumptions:**\n",
    "\n",
    "* We're using SQL Server for this example. Adapt the syntax for other databases like MySQL, Oracle, PostgreSQL accordingly.\n",
    "* Data types and constraints will depend on your specific requirements and database engine.\n",
    "* Indexes and other performance optimizations are not included in this basic script.\n",
    "\n",
    "**SQL Code:**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE Customers (\n",
    "    CustomerID INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    CustomerName VARCHAR(255),\n",
    "    Address VARCHAR(255),\n",
    "    City VARCHAR(255),\n",
    "    Postcode VARCHAR(255),\n",
    "    Country VARCHAR(255),\n",
    "    Phone VARCHAR(255),\n",
    "    Email VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Products (\n",
    "    StockCode VARCHAR(255) PRIMARY KEY,\n",
    "    Description VARCHAR(255),\n",
    "    UnitPrice DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "CREATE TABLE Orders (\n",
    "    OrderID INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    OrderDate DATE,\n",
    "    CustomerID INT FOREIGN KEY REFERENCES Customers(CustomerID),\n",
    "    ShipName VARCHAR(255),\n",
    "    ShipAddress VARCHAR(255),\n",
    "    ShipCity VARCHAR(255),\n",
    "    ShipPostcode VARCHAR(255),\n",
    "    ShipCountry VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE OrderDetails (\n",
    "    OrderID INT FOREIGN KEY REFERENCES Orders(OrderID),\n",
    "    StockCode VARCHAR(255) FOREIGN KEY REFERENCES Products(StockCode),\n",
    "    Quantity INT,\n",
    "    UnitPrice DECIMAL(10,2), -- Redundant, can be removed\n",
    "    Discount DECIMAL(10,2),\n",
    "    PRIMARY KEY (OrderID, StockCode) -- Composite primary key\n",
    ");\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Each table is created with its respective columns and data types.\n",
    "* Primary keys are defined for each table to ensure data integrity.\n",
    "* Foreign keys are established to link related tables (e.g., `CustomerID` in `Orders` references `CustomerID` in `Customers`).\n",
    "* A composite primary key is used for `OrderDetails` to uniquely identify each order item.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "* This is a basic structure. You may need to adjust data types, lengths, and constraints based on your specific data.\n",
    "* Consider adding indexes for frequently queried columns to improve performance.\n",
    "* For large datasets, partitioning might be necessary.\n",
    "* Data quality and cleansing should be implemented before loading data into these tables.\n",
    "* This schema doesn't include any dimensions or fact tables for analytical purposes. If you plan to build a data warehouse for analytics, a star or snowflake schema might be more suitable.\n",
    "\n",
    "Remember to replace placeholders like `VARCHAR(255)` with appropriate data types and lengths based on your data characteristics. Additionally, you might want to add constraints like `NOT NULL` for essential columns and default values where applicable.\n",
    "        \n",
    "**Would you like to explore creating a dimensional model for analytical purposes, or do you have any other specific requirements?**\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <img src=\"https://imgur.com/QnyoEKg.png\" alt=\"geminierd\"/>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Infrastructure Requirements__\n",
    "\n",
    "Let's frame a prompt that can use the context from the previous steps and create a infrastructure design requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "    <summary>GenAI prompt</summary>\n",
    "\n",
    "```python\n",
    "Propose a set of infrastructure requirements for different stages of the data workflow based on the data architecture discussed here\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary>GPT-4</summary>\n",
    "\n",
    "Certainly! Below is a set of infrastructure requirements for different stages of the data workflow based on the data architecture discussed earlier:\n",
    "\n",
    "### 1. Data Ingestion\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Servers**: Dedicated servers for running ETL tools and custom scripts.\n",
    "- **Storage**: High-capacity storage solutions (e.g., RAID arrays) for temporary storage of ingested data before it is loaded into the central repository.\n",
    "\n",
    "**Software Requirements:**\n",
    "- **ETL Tools**: Apache NiFi or Talend, installed and configured on dedicated servers.\n",
    "- **Custom Scripts**: Python or Java runtime environments for executing custom data ingestion scripts.\n",
    "- **Data Connectors**: Connectors for various data sources (e.g., JDBC connectors for databases, APIs for third-party services).\n",
    "\n",
    "### 2. Data Storage\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Database Servers**: High-performance servers with SSD storage for running the SQL database.\n",
    "- **Backup Storage**: Separate storage solutions for database backups (e.g., NAS, SAN).\n",
    "\n",
    "**Software Requirements:**\n",
    "- **SQL Database**: PostgreSQL or MySQL installed and configured on database servers.\n",
    "- **Replication Tools**: Tools for database replication to ensure high availability and redundancy.\n",
    "- **Backup Tools**: Tools for scheduling and managing regular database backups.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Processing Servers**: Servers with high CPU and memory capacity for batch and real-time data processing.\n",
    "- **Storage**: Additional storage for intermediate data generated during processing.\n",
    "\n",
    "**Software Requirements:**\n",
    "- **Batch Processing**: Cron jobs or Apache Airflow installed on processing servers for scheduling batch tasks.\n",
    "- **Real-Time Processing**: Apache Kafka or RabbitMQ for message queuing, and Apache Flink or Apache Storm for stream processing.\n",
    "\n",
    "### 4. Data Access\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Access Servers**: Servers for hosting BI tools and SQL query interfaces.\n",
    "- **Network Infrastructure**: High-speed network infrastructure to ensure quick data access for employees.\n",
    "\n",
    "**Software Requirements:**\n",
    "- **BI Tools**: On-premise versions of Metabase or Tableau installed on access servers.\n",
    "- **SQL Query Tools**: SQL clients (e.g., pgAdmin, MySQL Workbench) for employees to execute queries.\n",
    "- **Access Control Software**: Role-Based Access Control (RBAC) systems for managing data access permissions.\n",
    "\n",
    "### 5. Data Security\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Security Appliances**: Firewalls, intrusion detection systems (IDS), and intrusion prevention systems (IPS) for network security.\n",
    "- **Backup Servers**: Dedicated servers for storing encrypted backups.\n",
    "\n",
    "**Software Requirements:**\n",
    "- **Encryption Tools**: Tools for disk encryption (e.g., LUKS) and SSL/TLS for data in transit.\n",
    "- **Access Control Software**: RBAC systems integrated with the database.\n",
    "- **Audit Logging Tools**: Tools for maintaining and monitoring audit logs (e.g., ELK Stack).\n",
    "\n",
    "### 6. Data Governance\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **Governance Servers**: Servers for hosting data governance tools and applications.\n",
    "- **Storage**: Additional storage for data quality audits and compliance records.\n",
    "\n",
    "**Software Requirements:**\n",
    "- **Data Quality Tools**: Tools for implementing validation rules and conducting data quality audits.\n",
    "- **Compliance Tools**: Tools for ensuring regulatory compliance (e.g., data masking tools, compliance reporting tools).\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Data Ingestion**:\n",
    "   - **Hardware**: Dedicated servers, high-capacity storage.\n",
    "   - **Software**: ETL tools, custom scripts, data connectors.\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - **Hardware**: High-performance database servers, backup storage.\n",
    "   - **Software**: SQL database, replication tools, backup tools.\n",
    "\n",
    "3. **Data Processing**:\n",
    "   - **Hardware**: High-capacity processing servers, additional storage.\n",
    "   - **Software**: Batch processing tools, real-time processing tools.\n",
    "\n",
    "4. **Data Access**:\n",
    "   - **Hardware**: Access servers, high-speed network infrastructure.\n",
    "   - **Software**: BI tools, SQL query tools, access control software.\n",
    "\n",
    "5. **Data Security**:\n",
    "   - **Hardware**: Security appliances, backup servers.\n",
    "   - **Software**: Encryption tools, access control software, audit logging tools.\n",
    "\n",
    "6. **Data Governance**:\n",
    "   - **Hardware**: Governance servers, additional storage.\n",
    "   - **Software**: Data quality tools, compliance tools.\n",
    "\n",
    "These infrastructure requirements ensure that each stage of the data workflow is supported by the necessary hardware and software, providing a robust, secure, and efficient data management system for the online retail company.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "        <summary>Gemini</summary>\n",
    "\n",
    "## Infrastructure Requirements for Different Stages of the Data Workflow\n",
    "\n",
    "### Understanding the Data Workflow Stages\n",
    "\n",
    "Before diving into infrastructure requirements, let's clarify the data workflow stages:\n",
    "\n",
    "1. **Data Ingestion:** Collecting data from various sources (e.g., POS systems, e-commerce platforms, CRM).\n",
    "2. **Data Transformation:** Cleaning, standardizing, and enriching the data.\n",
    "3. **Data Loading:** Populating the data warehouse with transformed data.\n",
    "4. **Data Storage:** Storing the data in the data warehouse.\n",
    "5. **Data Processing:** Performing analytical queries and calculations.\n",
    "\n",
    "### Infrastructure Requirements\n",
    "\n",
    "#### Data Ingestion\n",
    "* **Data Sources:** Identify the types of data sources (e.g., databases, files, APIs).\n",
    "* **Data Volume and Velocity:** Assess the amount and speed of incoming data.\n",
    "* **Data Extraction Tools:** Consider tools like ETL/ELT tools (e.g., Informatica, Talend, Apache Airflow), data integration platforms, or custom scripts.\n",
    "* **Data Extraction Servers:** Servers with sufficient CPU, memory, and storage for running extraction processes.\n",
    "* **Network Connectivity:** Reliable network connections to data sources.\n",
    "\n",
    "#### Data Transformation\n",
    "* **Data Transformation Tools:** ETL/ELT tools, data integration platforms, or custom scripting.\n",
    "* **Data Transformation Servers:** Servers with sufficient CPU, memory, and storage for data transformation processes.\n",
    "* **Data Quality Tools:** Tools for data profiling, cleansing, and validation (optional).\n",
    "\n",
    "#### Data Loading\n",
    "* **Data Warehouse Servers:** High-performance servers with sufficient CPU, memory, and storage for the data warehouse database.\n",
    "* **Database Software:** Choose a suitable relational database management system (RDBMS) like SQL Server, Oracle, PostgreSQL, or cloud-based options like Amazon Redshift, Google BigQuery.\n",
    "* **Data Loading Tools:** ETL/ELT tools or bulk loading utilities provided by the database.\n",
    "\n",
    "#### Data Storage\n",
    "* **Data Warehouse Storage:** Storage infrastructure for the data warehouse database (e.g., SSDs, HDDs).\n",
    "* **Backup and Recovery:** Implement backup and recovery procedures to protect data.\n",
    "* **Data Archiving:** Consider archiving strategies for old data to reduce storage costs.\n",
    "\n",
    "#### Data Processing\n",
    "* **Data Warehouse Servers:** Sufficient CPU, memory, and storage for analytical queries.\n",
    "* **Business Intelligence Tools:** Tools for data visualization and reporting (e.g., Tableau, Power BI).\n",
    "* **Data Processing Servers:** Additional servers for complex analytical workloads (optional).\n",
    "\n",
    "### Additional Considerations\n",
    "\n",
    "* **Scalability:** Ensure infrastructure can handle increasing data volumes and processing demands.\n",
    "* **High Availability:** Implement redundancy and failover mechanisms.\n",
    "* **Security:** Protect data with encryption, access controls, and monitoring.\n",
    "* **Monitoring and Management:** Use monitoring tools to track system performance and resource utilization.\n",
    "* **Cloud vs. On-Premise:** Evaluate the benefits of cloud-based solutions (e.g., scalability, cost-efficiency, managed services) versus on-premise infrastructure.\n",
    "\n",
    "### Example Infrastructure Architecture\n",
    "\n",
    "\n",
    "\n",
    "**Note:** This is a general overview. Specific infrastructure requirements will vary based on factors such as data volume, complexity, performance requirements, and budget.\n",
    "\n",
    "**Would you like to delve deeper into a specific stage or infrastructure component?** \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ETL Workflow__\n",
    "\n",
    "\n",
    "At this point, you can assume that the central data collection has taken place and the invoice details are available as a CSV file on remote server. You need to create an ETL pipeline that reads this file, cleans the data based on the client specifications and loads the data to a SQL server. \n",
    "\n",
    "Specifications shared by the client:\n",
    "\n",
    "- InvoiceNo starting with the character C is a credit entry and should be removed from the record before analysis.\n",
    "\n",
    "- StockCode values of C2, D, M, and POST correspond to Carraige, Discount, Manual and Postage entries, all of which are not required for our analysis.\n",
    "\n",
    "- There are a few entries where the CustomerID is missing. Such entries can be removed from the data before your analysis.\n",
    "\n",
    "- Load the final transaction record to an SQLite3 database Invoice_Records under the table Purchase_transactions.\n",
    "\n",
    "- Create a prompt and use it on the generative AI platform to generate a Python code that can create the required data processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "   <summary>GenAI prompt</summary>\n",
    "\n",
    "```python\n",
    "For the transactional data table, create a python code that does the following.\n",
    "\n",
    "1. Read the CSV data from a URL using Pandas\n",
    "\n",
    "2. Remove entries where \n",
    "\n",
    "   a. InvoiceNo starts with the character 'C'\n",
    "\n",
    "   b. StockCode is either 'M', 'D', 'C2' or 'POST'\n",
    "\n",
    "   c. CustomerID is missing.\n",
    "   \n",
    "   Print the length of the dataset before and after removal.\n",
    "   \n",
    "\n",
    "3. Load the final data to an SQLIte3 database named 'Invoice_Records' as a table 'Purchase_transactions'.\n",
    "\n",
    "4. Run a sample query to display the first 5 rows of the table.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "        <summary>GPT-4</summary>\n",
    "\n",
    "#Certainly! Below is the Python code that accomplishes the tasks specified:\n",
    "\n",
    "```Python Code\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Read the CSV data from a URL using Pandas\n",
    "url = \"YOUR_CSV_URL_HERE\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Remove entries based on specified conditions\n",
    "# Print the length of the dataset before removal\n",
    "print(\"Length of dataset before removal:\", len(data))\n",
    "\n",
    "# a. Remove entries where InvoiceNo starts with the character 'C'\n",
    "data = data[~data['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "# b. Remove entries where StockCode is either 'M', 'D', 'C2' or 'POST'\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "\n",
    "# c. Remove entries where CustomerID is missing\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the dataset after removal\n",
    "print(\"Length of dataset after removal:\", len(data))\n",
    "\n",
    "# Step 3: Load the final data to an SQLite3 database named 'Invoice_Records' as a table 'Purchase_transactions'\n",
    "# Create a connection to the SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "\n",
    "# Load the data into the 'Purchase_transactions' table\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Step 4: Run a sample query to display the first 5 rows of the table\n",
    "query = \"SELECT * FROM Purchase_transactions LIMIT 5\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(result)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "```\n",
    "### Explanation:\n",
    "\n",
    "1. **Reading the CSV Data**:\n",
    "- The `pd.read_csv(url)` function reads the CSV data from the given URL into a Pandas DataFrame named `data`.\n",
    "\n",
    "2. **Removing Entries**:\n",
    "- The length of the dataset before removal is printed using `len(data)`.\n",
    "- Entries where `InvoiceNo` starts with 'C' are removed using `~data['InvoiceNo'].astype(str).str.startswith('C')`.\n",
    "- Entries where `StockCode` is either 'M', 'D', 'C2', or 'POST' are removed using `~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])`.\n",
    "- Entries where `CustomerID` is missing are removed using `data.dropna(subset=['CustomerID'])`.\n",
    "- The length of the dataset after removal is printed using `len(data)`.\n",
    "\n",
    "3. **Loading Data to SQLite3 Database**:\n",
    "- A connection to the SQLite3 database named `Invoice_Records.db` is created using `sqlite3.connect('Invoice_Records.db')`.\n",
    "- The final data is loaded into a table named `Purchase_transactions` using `data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)`.\n",
    "\n",
    "4. **Running a Sample Query**:\n",
    "- A sample SQL query to display the first 5 rows of the `Purchase_transactions` table is executed using `pd.read_sql_query(query, conn)`.\n",
    "- The result of the query is printed.\n",
    "- The database connection is closed using `conn.close()`.\n",
    "\n",
    "Replace `\"YOUR_CSV_URL_HERE\"` with the actual URL of your CSV file. This script will read the data, clean it according to the specified conditions, load it into an SQLite3 database, and display a sample of the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Gemini</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def process_transactional_data(url):\n",
    "  \"\"\"\n",
    "  Processes transactional data from a CSV URL.\n",
    "\n",
    "  Args:\n",
    "    url: The URL of the CSV file.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  # Step 1: Read CSV data using Pandas\n",
    "  df = pd.read_csv(url)\n",
    "\n",
    "  # Step 2: Remove unwanted entries\n",
    "  print(\"Original dataset length:\", len(df))\n",
    "  df = df[~df['InvoiceNo'].str.startswith('C')]\n",
    "  df = df[~df['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "  df = df[df['CustomerID'].notna()]\n",
    "  print(\"Dataset length after removal:\", len(df))\n",
    "\n",
    "  # Step 3: Load data to SQLite database\n",
    "  conn = sqlite3.connect('Invoice_Records.db')\n",
    "  df.to_sql('Purchase_transactions', conn, index=False, if_exists='replace')\n",
    "  conn.close()\n",
    "\n",
    "   # Step 4: Run sample query\n",
    "  conn = sqlite3.connect('Invoice_Records.db')\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute(\"SELECT * FROM Purchase_transactions LIMIT 5\")\n",
    "  rows = cursor.fetchall()\n",
    "  print(rows)\n",
    "  conn.close()\n",
    "```\n",
    "# Example usage:\n",
    "#url = 'https://your_csv_file_url'  # Replace with your actual CSV URL\n",
    "#process_transactional_data(url)\n",
    "#Use code with caution.\n",
    "#    content_copy\n",
    "#    Explanation:\n",
    "#    Import necessary libraries: Imports pandas for data manipulation and sqlite3 for database interaction.\n",
    "#    Define the function: Creates a function process_transactional_data to encapsulate the process.\n",
    "#    Read CSV data: Reads the CSV data from the specified URL into a Pandas DataFrame.\n",
    "#    Data cleaning: Removes unwanted entries based on the given criteria.\n",
    "#    Create SQLite database: Creates a SQLite database named 'Invoice_Records'.\n",
    "#    Load data to database: Loads the cleaned DataFrame into a table named 'Purchase_transactions' in the database.\n",
    "#    Sample query: Executes a query to display the first 5 rows of the table.\n",
    "#    Notes:\n",
    "#    Replace https://your_csv_file_url with the actual URL of your CSV file.\n",
    "#    Adjust data types and column names in the DataFrame and SQL table creation as needed.\n",
    "#    Consider error handling for file reading, database connections, and query execution.\n",
    "#    For large datasets, consider using optimized data types and indexing in the database.\n",
    "#    This code provides a basic framework for the specified task. Additional refinements can be made based on specific requirements and data characteristics.\n",
    "\n",
    "#    Would you like to add more functionalities to this code, such as data validation or specific calculations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset before removal: 24180\n",
      "Length of dataset after removal: 22420\n",
      "  InvoiceNo StockCode                      Description  Quantity  \\\n",
      "0    536370     22728        ALARM CLOCK BAKELIKE PINK        24   \n",
      "1    536370     22727        ALARM CLOCK BAKELIKE RED         24   \n",
      "2    536370     22726       ALARM CLOCK BAKELIKE GREEN        12   \n",
      "3    536370     21724  PANDA AND BUNNIES STICKER SHEET        12   \n",
      "4    536370     21883                 STARS GIFT TAPE         24   \n",
      "\n",
      "        InvoiceDate  UnitPrice  CustomerID Country  \n",
      "0  01-12-2010 08:45       3.75     12583.0  France  \n",
      "1  01-12-2010 08:45       3.75     12583.0  France  \n",
      "2  01-12-2010 08:45       3.75     12583.0  France  \n",
      "3  01-12-2010 08:45       0.85     12583.0  France  \n",
      "4  01-12-2010 08:45       0.65     12583.0  France  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Read the CSV data from a URL using Pandas\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Remove entries based on specified conditions\n",
    "# Print the length of the dataset before removal\n",
    "print(\"Length of dataset before removal:\", len(data))\n",
    "\n",
    "# a. Remove entries where InvoiceNo starts with the character 'C'\n",
    "data = data[~data['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "# b. Remove entries where StockCode is either 'M', 'D', 'C2' or 'POST'\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "\n",
    "# c. Remove entries where CustomerID is missing\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the dataset after removal\n",
    "print(\"Length of dataset after removal:\", len(data))\n",
    "\n",
    "# Step 3: Load the final data to an SQLite3 database named 'Invoice_Records' as a table 'Purchase_transactions'\n",
    "# Create a connection to the SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "\n",
    "# Load the data into the 'Purchase_transactions' table\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Step 4: Run a sample query to display the first 5 rows of the table\n",
    "query = \"SELECT * FROM Purchase_transactions LIMIT 5\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(result)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Database Queries__\n",
    "After the data is available on the central data repository, you can use SQL queries to extract the data directly into your Python coding interface. For the next part of your project, you are required to extract the data of a specific country–let's use Spain. Use the generative AI model to create a code snippet that you can add to your previous code in the testing interface and that will run a SQL query on the data to extract the details transactions for the specified country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "        <summary>GenAI Prompt</summary>\n",
    "\n",
    "```python\n",
    "Create a python code that will extract all records from the table 'Purchase_transactions' under the database 'Invoice_records', such that the Country parameter is set to Spain. The resulting record should be saved in a dataframe.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>GPT-4</summary>\n",
    "\n",
    "Certainly! Below is the Python code that extracts all records from the `Purchase_transactions` table in the `Invoice_Records` database where the `Country` parameter is set to 'Spain'. The resulting records are saved in a Pandas DataFrame.\n",
    "\n",
    "```Python Code\n",
    "\n",
    "# ```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Create a connection to the SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "\n",
    "# Step 2: Define the SQL query to extract records where Country is 'Spain'\n",
    "query = \"SELECT * FROM Purchase_transactions WHERE Country = 'Spain'\"\n",
    "\n",
    "# Step 3: Execute the query and save the result in a DataFrame\n",
    "df_spain = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 4: Print the resulting DataFrame\n",
    "print(df_spain)\n",
    "\n",
    "# Step 5: Close the database connection\n",
    "conn.close() \n",
    "```\n",
    "\n",
    "## Explanation:\n",
    "\n",
    "1. **Creating a Connection**:\n",
    "    - A connection to the SQLite3 database named `Invoice_Records.db` is created using `sqlite3.connect('Invoice_Records.db')`.\n",
    "\n",
    "2. **Defining the SQL Query**:\n",
    "    - The SQL query to extract records where the `Country` parameter is set to 'Spain' is defined as `query = \"SELECT * FROM Purchase_transactions WHERE Country = 'Spain'\"`.\n",
    "\n",
    "3. **Executing the Query**:\n",
    "    - The query is executed and the result is saved in a Pandas DataFrame named `df_spain` using `pd.read_sql_query(query, conn)`.\n",
    "\n",
    "4. **Printing the DataFrame**:\n",
    "    - The resulting DataFrame is printed using `print(df_spain)`.\n",
    "\n",
    "5. **Closing the Database Connection**:\n",
    "    - The database connection is closed using `conn.close()`.\n",
    "\n",
    "This script will connect to the `Invoice_Records` database, extract the relevant records from the `Purchase_transactions` table where the `Country` is 'Spain', and save the resulting records in a Pandas DataFrame.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Gemini</summary>\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def extract_spanish_transactions(db_file):\n",
    "  \"\"\"\n",
    "  Extracts records from the 'Purchase_transactions' table where 'Country' is 'Spain'.\n",
    "\n",
    "  Args:\n",
    "    db_file: The path to the SQLite database file.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame containing the extracted records.\n",
    "  \"\"\"\n",
    "\n",
    "  conn = sqlite3.connect(db_file)\n",
    "  query = \"SELECT * FROM Purchase_transactions WHERE Country = 'Spain'\"\n",
    "  df = pd.read_sql_query(query, conn)\n",
    "  conn.close()\n",
    "  return df\n",
    "\n",
    "# Example usage:\n",
    "db_file = 'Invoice_Records.db'\n",
    "spanish_transactions = extract_spanish_transactions(db_file)\n",
    "print(spanish_transactions)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **Import necessary libraries:** Imports `sqlite3` for database interaction and `pandas` for DataFrame manipulation.\n",
    "2. **Define the function:** Creates a function `extract_spanish_transactions` to encapsulate the extraction process.\n",
    "3. **Connect to the database:** Establishes a connection to the SQLite database using the provided `db_file` path.\n",
    "4. **Execute the query:** Constructs an SQL query to select all records from the 'Purchase_transactions' table where the 'Country' column equals 'Spain'.\n",
    "5. **Create DataFrame:** Reads the query results into a pandas DataFrame.\n",
    "6. **Close the connection:** Closes the database connection.\n",
    "7. **Return the DataFrame:** Returns the DataFrame containing the extracted records.\n",
    "\n",
    "This code effectively extracts the desired records and stores them in a pandas DataFrame for further analysis or processing.\n",
    " \n",
    "**Would you like to add more functionalities to this code, such as filtering by additional criteria or performing calculations on the extracted data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     InvoiceNo StockCode                    Description  Quantity  \\\n",
      "0       536944     22383        LUNCH BAG SUKI  DESIGN         70   \n",
      "1       536944     22384        LUNCH BAG PINK POLKADOT       100   \n",
      "2       536944     20727        LUNCH BAG  BLACK SKULL.        60   \n",
      "3       536944     20725        LUNCH BAG RED RETROSPOT        70   \n",
      "4       536944     20728            LUNCH BAG CARS BLUE       100   \n",
      "...        ...       ...                            ...       ...   \n",
      "2418    581193     23291       DOLLY GIRL CHILDRENS CUP         2   \n",
      "2419    581193    85232D  SET/3 DECOUPAGE STACKING TINS         1   \n",
      "2420    581193     22721  SET OF 3 CAKE TINS SKETCHBOOK         2   \n",
      "2421    581193     23241   TREASURE TIN GYMKHANA DESIGN         1   \n",
      "2422    581193     23247     BISCUIT TIN 50'S CHRISTMAS         1   \n",
      "\n",
      "           InvoiceDate  UnitPrice  CustomerID Country  \n",
      "0     03-12-2010 12:20       1.65     12557.0   Spain  \n",
      "1     03-12-2010 12:20       1.45     12557.0   Spain  \n",
      "2     03-12-2010 12:20       1.65     12557.0   Spain  \n",
      "3     03-12-2010 12:20       1.65     12557.0   Spain  \n",
      "4     03-12-2010 12:20       1.45     12557.0   Spain  \n",
      "...                ...        ...         ...     ...  \n",
      "2418  07-12-2011 17:05       1.25     17097.0   Spain  \n",
      "2419  07-12-2011 17:05       4.95     17097.0   Spain  \n",
      "2420  07-12-2011 17:05       1.95     17097.0   Spain  \n",
      "2421  07-12-2011 17:05       2.08     17097.0   Spain  \n",
      "2422  07-12-2011 17:05       2.89     17097.0   Spain  \n",
      "\n",
      "[2423 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#import sqlite3\n",
    "\n",
    "# Step 1: Create a connection to the SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "\n",
    "# Step 2: Define the SQL query to extract records where Country is 'Spain'\n",
    "query = \"SELECT * FROM Purchase_transactions WHERE Country = 'Spain'\"\n",
    "\n",
    "# Step 3: Execute the query and save the result in a DataFrame\n",
    "df_spain = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 4: Print the resulting DataFrame\n",
    "print(df_spain)\n",
    "\n",
    "# Step 5: Close the database connection\n",
    "conn.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Analysis and Mining__\n",
    "\n",
    "\n",
    "One of the most relevant and important techniques for analyzing transactional data is [association rule mining](https://www.google.com/search?q=association+rule+mining&rlz=1C1CHBF_enAO733AO733&oq=association+rule+mining&gs_lcrp=EgZjaHJvbWUyBwgAEEUYgAQyBwgBEAAYgAQyBwgCEAAYgAQyBwgDEAAYgAQyBwgEEAAYgAQyBwgFEAAYgAQyBwgGEAAYgAQyBwgHEAAYgAQyBwgIEAAYgAQyBwgJEAAYgATSAQkxMzM5N2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8), You are required to implement an [Apriori algorithm](https://www.google.com/search?q=apriori+algorithm&sca_esv=cbc386de4b870031&rlz=1C1CHBF_enAO733AO733&sxsrf=ADLYWIJnICEn_ffpiwx0IT6sSHVOthXW6w%3A1720740183460&ei=V2mQZp7pG5KmkdUPzZ-U8Ak&oq=apriorio&gs_lp=Egxnd3Mtd2l6LXNlcnAiCGFwcmlvcmlvKgIIADIKEAAYgAQYQxiKBTIHEAAYgAQYCjINEC4YgAQYxwEYChivATIHEAAYgAQYCjIKEAAYgAQYQxiKBTIKEAAYgAQYQxiKBTIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkioNVDSE1jWLnAIeAGQAQCYAaoCoAGZCaoBBTEuNy4xuAEDyAEA-AEBmAIRoALdCsICChAAGLADGNYEGEfCAg0QABiABBiwAxhDGIoFwgIKECMYgAQYJxiKBcICBRAAGIAEwgILEC4YgAQY0QMYxwHCAgUQLhiABMICCxAuGIAEGMcBGK8BwgIIEAAYgAQYogSYAwCIBgGQBgqSBwc4LjguMC4xoAfkUw&sclient=gws-wiz-serp) to mine association rules from the data extracted in the previous step. As a result, you will be able to identify the items that have the most likelihood to be purchased together. This information is necessary for the company to efficiently develop their marketing and advertising strategies.\n",
    "\n",
    "You can use generative AI to create the code for implementation. You can write a prompt that creates code for the implementation of Apriori algorithm for association rule mining on the extracted data frame.\n",
    "\n",
    "Consider the following prompt that describes the different steps involved in implementing an Apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "        <summary>GenAI Prompt</summary>\n",
    "\n",
    "```python\n",
    "For the data frame extracted here, write a python code to execute the apriori algorithm and extract association rules for the given set of invoices. For this outcome, perform the following steps:\n",
    "\n",
    "Group the records by InvoiceID and Description, along with their total quantities\n",
    "\n",
    "Unpack the data into a table, making the InvoiceNo, the row indexes, Unique descriptions as column indexes and the total quantities, the values.\n",
    "\n",
    "Apply one-hot encoding on this table, making the value True if the Item description existed in the invoice and False if it didn't exist in the invoice.\n",
    "\n",
    "Perfrom Apriori algorithm on this data and extract the required association rules.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>GPT-4</summary>\n",
    "Certainly! Below is the Python code to execute the Apriori algorithm and extract association rules from the given set of invoices. The steps include grouping the records, unpacking the data, applying one-hot encoding, and then applying the Apriori algorithm.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 1: Group the records by InvoiceNo and Description, along with their total quantities\n",
    "grouped_df = df_spain.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().fillna(0)\n",
    "\n",
    "# Step 2: Unpack the data into a table\n",
    "# InvoiceNo as row indexes, unique descriptions as column indexes, and total quantities as values\n",
    "# This is already achieved by the unstack() function in the previous step\n",
    "\n",
    "# Step 3: Apply one-hot encoding\n",
    "# Making the value True if the Item description existed in the invoice and False if it didn't exist\n",
    "one_hot_df = grouped_df.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Step 4: Perform Apriori algorithm on this data\n",
    "# Set minimum support threshold (e.g., 0.01)\n",
    "frequent_itemsets = apriori(one_hot_df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Extract the required association rules\n",
    "# Set minimum confidence threshold (e.g., 0.5)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Print the resulting rules\n",
    "print(rules)\n",
    "\n",
    "# Optional: Save the rules to a CSV file\n",
    "rules.to_csv('association_rules.csv', index=False)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Grouping the Records**:\n",
    "    - The records are grouped by `InvoiceNo` and `Description`, and their total quantities are summed using `grouped_df = df_spain.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().fillna(0)`.\n",
    "    - The `unstack()` function transforms the data so that `InvoiceNo` becomes the row index, unique `Description` values become the column indexes, and the total quantities become the values.\n",
    "    - The `fillna(0)` method fills any missing values with 0.\n",
    "\n",
    "2. **Unpacking the Data**:\n",
    "    - The data is already unpacked into the required table format by the `unstack()` function in the previous step.\n",
    "\n",
    "3. **One-Hot Encoding**:\n",
    "    - One-hot encoding is applied using `one_hot_df = grouped_df.applymap(lambda x: 1 if x > 0 else 0)`, which converts the quantities into binary values (1 if the item exists in the invoice, 0 if it does not).\n",
    "\n",
    "4. **Applying the Apriori Algorithm**:\n",
    "    - The Apriori algorithm is applied using `frequent_itemsets = apriori(one_hot_df, min_support=0.01, use_colnames=True)`, with a minimum support threshold of 0.01.\n",
    "    - Association rules are extracted using `rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)`, with a minimum confidence threshold of 0.5.\n",
    "    - The resulting rules are printed using `print(rules)`.\n",
    "    - Optionally, the rules can be saved to a CSV file using `rules.to_csv('association_rules.csv', index=False)`.\n",
    "\n",
    "This script will group the data, apply one-hot encoding, run the Apriori algorithm, and extract association rules based on the specified thresholds. Adjust the `min_support` and `min_threshold` values as needed for your specific dataset and analysis requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Gemini</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "def apriori_analysis(df):\n",
    "  \"\"\"\n",
    "  Performs Apriori analysis on the given DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: A pandas DataFrame containing the transaction data.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame containing the association rules.\n",
    "  \"\"\"\n",
    "\n",
    "  # Step 1: Group by InvoiceNo and Description, summing quantities\n",
    "  basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "\n",
    "  # Step 2: Apply one-hot encoding\n",
    "  encoded_df = (basket > 0).astype(int)\n",
    "\n",
    "  # Step 3: Apply Apriori algorithm\n",
    "  frequent_itemsets = apriori(encoded_df, min_support=0.05, use_colnames=True)  # Adjust min_support as needed\n",
    "  rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)\n",
    "\n",
    "  return rules\n",
    "\n",
    "# Example usage:\n",
    "# Assuming german_transactions is the DataFrame from the previous response\n",
    "association_rules = apriori_analysis(german_transactions)\n",
    "print(association_rules)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Import necessary libraries:** Imports `pandas` for data manipulation, `apriori` and `association_rules` from `mlxtend` for association rule mining.\n",
    "2. **Define the function:** Creates a function `apriori_analysis` to encapsulate the analysis process.\n",
    "3. **Group and reshape data:**\n",
    "   * Groups the DataFrame by `InvoiceNo` and `Description`, summing the quantities.\n",
    "   * Unstacks the data to create a wide format with `InvoiceNo` as index, `Description` as columns, and quantities as values.\n",
    "   * Fills missing values with 0.\n",
    "4. **Apply one-hot encoding:** Converts the DataFrame to a binary matrix indicating the presence or absence of each product in each invoice.\n",
    "5. **Apply Apriori algorithm:** Uses the `apriori` function to find frequent itemsets with a minimum support of 0.05 (adjust as needed). Then, generates association rules using the `association_rules` function with a minimum lift threshold of 1.\n",
    "6. **Return association rules:** Returns the DataFrame containing the association rules.\n",
    "\n",
    "### Notes:\n",
    "* The `min_support` and `min_threshold` parameters in the `apriori` and `association_rules` functions can be adjusted based on the desired level of significance.\n",
    "* This code assumes that the `german_transactions` DataFrame has columns `InvoiceNo`, `Description`, and `Quantity`. Adjust column names if necessary.\n",
    "* For large datasets, consider performance optimizations like using sparse matrices or parallel processing.\n",
    "\n",
    "This code provides a basic framework for performing Apriori analysis on the extracted transaction data. You can further explore the generated association rules by examining the `antecedents`, `consequents`, `support`, `confidence`, and `lift` columns in the resulting DataFrame.\n",
    " \n",
    "**Would you like to explore additional metrics for evaluating association rules or visualize the results?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Putting it all together, we end up with a similar version of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         antecedents  \\\n",
      "6  (SET/20 RED RETROSPOT PAPER NAPKINS , SET/6 RE...   \n",
      "5  (SET/20 RED RETROSPOT PAPER NAPKINS , SET/6 RE...   \n",
      "2                    (SET/6 RED SPOTTY PAPER PLATES)   \n",
      "3                      (SET/6 RED SPOTTY PAPER CUPS)   \n",
      "4  (PLASTERS IN TIN SPACEBOY, PLASTERS IN TIN CIR...   \n",
      "0               (ROUND SNACK BOXES SET OF 4 FRUITS )   \n",
      "7  (SET/6 RED SPOTTY PAPER PLATES, SET/6 RED SPOT...   \n",
      "1                    (SET/6 RED SPOTTY PAPER PLATES)   \n",
      "\n",
      "                             consequents  confidence  \n",
      "6        (SET/6 RED SPOTTY PAPER PLATES)    0.964912  \n",
      "5          (SET/6 RED SPOTTY PAPER CUPS)    0.916667  \n",
      "2          (SET/6 RED SPOTTY PAPER CUPS)    0.904762  \n",
      "3        (SET/6 RED SPOTTY PAPER PLATES)    0.883721  \n",
      "4     (PLASTERS IN TIN WOODLAND ANIMALS)    0.764706  \n",
      "0  (ROUND SNACK BOXES SET OF4 WOODLAND )    0.750000  \n",
      "7  (SET/20 RED RETROSPOT PAPER NAPKINS )    0.723684  \n",
      "1  (SET/20 RED RETROSPOT PAPER NAPKINS )    0.714286  \n"
     ]
    }
   ],
   "source": [
    "# revised code\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Read the CSV data from a URL\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Print the length of the dataset before removal\n",
    "#print(\"Length of data set before removal:\", len(data))\n",
    "\n",
    "# Remove entries that meet the specified conditions\n",
    "initial_length = len(data)\n",
    "data = data[~data['InvoiceNo'].str.startswith('C')]\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the data set after removal\n",
    "#print(\"Length of data set after removal:\", len(data))\n",
    "#print(\"Number of entries removed:\", initial_length - len(data))\n",
    "\n",
    "# Load the final data to an SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Run a sample query to display the first 5 rows of the table\n",
    "query_result = pd.read_sql_query(\"SELECT * FROM Purchase_transactions LIMIT 5;\", conn)\n",
    "#print(query_result)\n",
    "\n",
    "query = \"SELECT * FROM Purchase_transactions WHERE Country IN ('Spain')\"\n",
    "records = pd.read_sql(query, conn)\n",
    "\n",
    "# Print the extracted records\n",
    "#print(records)\n",
    "\n",
    "# Execute a query to select all records from the 'Purchase_transactions' table\n",
    "query = \"SELECT InvoiceNo, Description, SUM(Quantity) AS TotalQuantity FROM Purchase_transactions GROUP BY InvoiceNo, Description\"\n",
    "df_grouped = pd.read_sql(query, conn)\n",
    "\n",
    "df_pivot = df_grouped.pivot(index='InvoiceNo', columns='Description', values='TotalQuantity').fillna(0)\n",
    "\n",
    "df_encoded = df_pivot.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Sort the association rules in descending order of confidence\n",
    "rules = rules.sort_values(by='confidence', ascending=False)\n",
    "\n",
    "# Print the association rules\n",
    "print((rules[['antecedents','consequents','confidence']]))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data, we can infer that selecting an item from the ‘antecedent’ column often leads to the subsequent selection of the associated item in the ‘consequent’ column during the same purchase. \n",
    "This insight can empower business leaders to devise more effective and refined strategic marketing campaigns. \n",
    "Ultimately, this data-driven approach can enhance the organization’s decision-making process in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
